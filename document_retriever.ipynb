{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T19:42:11.843834923Z",
     "start_time": "2023-11-22T19:42:11.836865430Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:42:13.076808265Z",
     "start_time": "2023-11-22T19:42:13.063417014Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_documents(url, categories, medical): \n",
    "    for c in categories:\n",
    "        print(c)\n",
    "        params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'cmtitle': c,\n",
    "                'cmlimit': '100',\n",
    "                'cmtype': 'page',\n",
    "                'list': 'categorymembers',\n",
    "        }\n",
    "        \n",
    "        req = requests.get(url=url, params=params)\n",
    "        pages = req.json()[\"query\"][\"categorymembers\"]\n",
    "        \n",
    "        page_ids = [page[\"pageid\"] for page in pages]\n",
    "        print(len(page_ids))\n",
    "        \n",
    "        for id in page_ids:\n",
    "            print(f\"Scraping page: {id}\")\n",
    "            content = get_content(url, id)\n",
    "            filename = f\"Corpora/Medical/{id}.txt\" if medical else f\"Corpora/NonMedical/{id}.txt\"\n",
    "            with open(filename, \"w\") as file:\n",
    "                file.write(content)\n",
    "\n",
    "def get_content(url, id):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"pageids\": id,\n",
    "        \"explaintext\" : \"1\",\n",
    "    }\n",
    "            \n",
    "    req = requests.get(url=url, params=params)\n",
    "    content = req.json()[\"query\"][\"pages\"][str(id)][\"extract\"]\n",
    "    \n",
    "    return content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:54:21.940579303Z",
     "start_time": "2023-11-22T19:54:21.899589864Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_documents():\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    \n",
    "    medical_categories = [\n",
    "        \"Category:Bacteriology\",\n",
    "        \"Category:Virology\",\n",
    "        \"Category:Cancer\",\n",
    "        \"Category:Anatomy\",\n",
    "        \"Category:Genetics\",\n",
    "        \"Category:Pediatrics\",\n",
    "    ]\n",
    "    \n",
    "    non_medical_categories = [\n",
    "        \"Category:Geometry\",\n",
    "        \"Category:Literature\",\n",
    "        \"Category:Hunting\",\n",
    "        \"Category:Politics\",\n",
    "        \"Category:Education\",\n",
    "        \"Category:Fashion\",\n",
    "    ]\n",
    "    \n",
    "    save_documents(url, medical_categories, medical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:54:59.072458972Z",
     "start_time": "2023-11-22T19:54:23.868455853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Bacteriology\n",
      "100\n",
      "Scraping page: 42766846\n",
      "Scraping page: 58475\n",
      "Scraping page: 2644968\n",
      "Scraping page: 10323278\n",
      "Scraping page: 6148256\n",
      "Scraping page: 33821697\n",
      "Scraping page: 28066105\n",
      "Scraping page: 8868378\n",
      "Scraping page: 39653832\n",
      "Scraping page: 9028799\n",
      "Scraping page: 53190588\n",
      "Scraping page: 2580748\n",
      "Scraping page: 1368466\n",
      "Scraping page: 4209093\n",
      "Scraping page: 4460\n",
      "Scraping page: 40138\n",
      "Scraping page: 35547268\n",
      "Scraping page: 17575156\n",
      "Scraping page: 22938418\n",
      "Scraping page: 1110607\n",
      "Scraping page: 16615241\n",
      "Scraping page: 33821608\n",
      "Scraping page: 33821528\n",
      "Scraping page: 67479291\n",
      "Scraping page: 33821580\n",
      "Scraping page: 33821481\n",
      "Scraping page: 33821340\n",
      "Scraping page: 52791986\n",
      "Scraping page: 34067938\n",
      "Scraping page: 47852064\n",
      "Scraping page: 20815112\n",
      "Scraping page: 3690837\n",
      "Scraping page: 43946\n",
      "Scraping page: 42689156\n",
      "Scraping page: 61641456\n",
      "Scraping page: 56209785\n",
      "Scraping page: 45523355\n",
      "Scraping page: 42326835\n",
      "Scraping page: 25162968\n",
      "Scraping page: 31882943\n",
      "Scraping page: 4527097\n",
      "Scraping page: 34690814\n",
      "Scraping page: 55750470\n",
      "Scraping page: 17575142\n",
      "Scraping page: 52709623\n",
      "Scraping page: 28325694\n",
      "Scraping page: 37536043\n",
      "Scraping page: 62164190\n",
      "Scraping page: 5207843\n",
      "Scraping page: 31566839\n",
      "Scraping page: 16910493\n",
      "Scraping page: 889890\n",
      "Scraping page: 42957\n",
      "Scraping page: 34888669\n",
      "Scraping page: 31556912\n",
      "Scraping page: 45680415\n",
      "Scraping page: 21734002\n",
      "Scraping page: 6862740\n",
      "Scraping page: 43812756\n",
      "Scraping page: 13575891\n",
      "Scraping page: 5510937\n",
      "Scraping page: 9704008\n",
      "Scraping page: 57805865\n",
      "Scraping page: 2839182\n",
      "Scraping page: 9894237\n",
      "Scraping page: 9939778\n",
      "Scraping page: 12935\n",
      "Scraping page: 12937\n",
      "Scraping page: 12936\n",
      "Scraping page: 44286894\n",
      "Scraping page: 3135637\n",
      "Scraping page: 397470\n",
      "Scraping page: 12361147\n",
      "Scraping page: 205464\n",
      "Scraping page: 16359310\n",
      "Scraping page: 41815686\n",
      "Scraping page: 6250168\n",
      "Scraping page: 66978170\n",
      "Scraping page: 867927\n",
      "Scraping page: 39665749\n",
      "Scraping page: 19158860\n",
      "Scraping page: 33992713\n",
      "Scraping page: 5614737\n",
      "Scraping page: 43785666\n",
      "Scraping page: 5391044\n",
      "Scraping page: 2720799\n",
      "Scraping page: 39821102\n",
      "Scraping page: 29473124\n",
      "Scraping page: 46500642\n",
      "Scraping page: 38522267\n",
      "Scraping page: 20814826\n",
      "Scraping page: 10085290\n",
      "Scraping page: 2954908\n",
      "Scraping page: 50221398\n",
      "Scraping page: 33647136\n",
      "Scraping page: 1057083\n",
      "Scraping page: 19456032\n",
      "Scraping page: 476265\n",
      "Scraping page: 66753944\n",
      "Scraping page: 63419161\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:47:30.353666485Z",
     "start_time": "2023-11-22T19:47:30.312055195Z"
    }
   },
   "outputs": [],
   "source": [
    "# popoulates the test set subtrating the 20% of training set elements\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def populate_test_set(): \n",
    "    counts = [0, 0]\n",
    "    medical_documents = os.listdir('Corpora/Medical')\n",
    "    non_medical_documents = os.listdir('Corpora/NonMedical')\n",
    "    \n",
    "    counts[0] = len(os.listdir('Corpora/NonMedical'))\n",
    "    counts[1] = len(os.listdir('Corpora/Medical'))\n",
    "    \n",
    "    numer_of_documents = np.sum(counts)\n",
    "    probabilities = [counts[0]/numer_of_documents, counts[1]/numer_of_documents]\n",
    "    \n",
    "    for _ in range(int(numer_of_documents*0.2)):\n",
    "        medical = np.random.choice([False, True], p=probabilities)\n",
    "        \n",
    "        document_index = None\n",
    "        \n",
    "        if medical:\n",
    "            document_index = np.random.randint(0, counts[1])\n",
    "        else:\n",
    "            document_index = np.random.randint(0, counts[0])\n",
    "            \n",
    "        if medical:\n",
    "            shutil.move(f\"Corpora/Medical/{medical_documents[document_index]}\", f\"Test/TestSet/{medical_documents[document_index]}\")\n",
    "            with open(\"Test/test_labels.txt\", \"a\") as f:\n",
    "                f.write(\"1\\n\")\n",
    "            del medical_documents[document_index]\n",
    "            counts[1] -= 1\n",
    "        else:\n",
    "            shutil.move(f\"Corpora/NonMedical/{non_medical_documents[document_index]}\", f\"Test/TestSet/{non_medical_documents[document_index]}\")\n",
    "            with open(\"Test/test_labels.txt\", \"a\") as f:\n",
    "                f.write(\"0\\n\")\n",
    "            del non_medical_documents[document_index]\n",
    "            counts[0] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:55:23.368754790Z",
     "start_time": "2023-11-22T19:55:23.299898983Z"
    }
   },
   "outputs": [],
   "source": [
    "populate_test_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
