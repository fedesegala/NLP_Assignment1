{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T17:50:11.709725423Z",
     "start_time": "2023-11-16T17:50:11.696105594Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def save_documents(url, categories, medical): \n",
    "    for c in categories:\n",
    "        print(c)\n",
    "        params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'cmtitle': c,\n",
    "                'cmlimit': '100',\n",
    "                'cmtype': 'page',\n",
    "                'list': 'categorymembers',\n",
    "        }\n",
    "        \n",
    "        req = requests.get(url=url, params=params)\n",
    "        pages = req.json()[\"query\"][\"categorymembers\"]\n",
    "        \n",
    "        page_ids = [page[\"pageid\"] for page in pages]\n",
    "        \n",
    "        for id in page_ids:\n",
    "            print(f\"Scraping page: {id}\")\n",
    "            content = get_content(url, id)\n",
    "            filename = f\"Corpora/Medical/{id}.txt\" if medical else f\"Corpora/NonMedical/{id}.txt\"\n",
    "            with open(filename, \"w\") as file:\n",
    "                file.write(content)\n",
    "\n",
    "def get_content(url, id):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"pageids\": id,\n",
    "        \"explaintext\" : \"1\",\n",
    "    }\n",
    "            \n",
    "    req = requests.get(url=url, params=params)\n",
    "    content = req.json()[\"query\"][\"pages\"][str(id)][\"extract\"]\n",
    "    \n",
    "    return content\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:50:11.741306510Z",
     "start_time": "2023-11-16T17:50:11.699344894Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def retrieve_documents():\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    \n",
    "    medical_categories = [\n",
    "        \"Category:Bacteriology\",\n",
    "        \"Category:Virology\",\n",
    "        \"Category:Cancer\",\n",
    "        \"Category:Anatomy\",\n",
    "        \"Category:Genetics\",\n",
    "        \"Category:Pediatrics\",\n",
    "    ]\n",
    "    \n",
    "    non_medical_categories = [\n",
    "        \"Category:Culture\",\n",
    "        \"Category:Literature\",\n",
    "        \"Category:Hunting\",\n",
    "        \"Category:Politics\",\n",
    "        \"Category:Fashion\"\n",
    "        \"Category:Architecture\"\n",
    "    ]\n",
    "    \n",
    "    save_documents(url, non_medical_categories, medical=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:12:35.684925076Z",
     "start_time": "2023-11-16T19:12:35.644369017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def bag_of_words(path):\n",
    "    BoW = {}\n",
    "    \n",
    "    files = sorted(glob.glob(path))\n",
    "    \n",
    "    for file in files: \n",
    "        with open(file, \"r\") as f: \n",
    "            data = f.read()\n",
    "            \n",
    "            current_bag_of_words = normalize(data)\n",
    "            \n",
    "            # merge this bag of word into the category bag_of_word\n",
    "            for word, count in current_bag_of_words.items():\n",
    "                if word not in BoW.keys():\n",
    "                    BoW.update({word:count})\n",
    "                else:\n",
    "                    BoW.update({word:(BoW.get(word) + count)})\n",
    "    \n",
    "    return BoW"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:14.526546059Z",
     "start_time": "2023-11-16T19:38:14.521760779Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Normalizes a file by means of tokenization, stemming, stopwords elimination, returning its representation as a Bag of Words\n",
    "    \n",
    "    :param data: the file we want to normalize represented as a string\n",
    "    :return: the bag of words representation of the input file  \n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    file_bag_of_words = {}\n",
    "    \n",
    "    tokens = tokenizer.tokenize(data)   # tokenization\n",
    "    \n",
    "    for token in tokens:    # for each token check that it is significant (not a stopword and longer than 3)\n",
    "        if token not in stopwords_list and len(token) > 3:\n",
    "            stem = stemmer.stem(word=token, to_lowercase=True)  # stemming\n",
    "            \n",
    "            # the string '== Section Name ==' is used to divide sections, don't want to include this tokens\n",
    "            if '=' not in stem:\n",
    "                if stem not in file_bag_of_words:\n",
    "                    file_bag_of_words.update({stem: 1})\n",
    "                else:\n",
    "                    value = file_bag_of_words.get(stem)\n",
    "                    file_bag_of_words.update({stem: value+1})\n",
    "    \n",
    "    return file_bag_of_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:16.611897841Z",
     "start_time": "2023-11-16T19:38:16.604863130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def classify(path, vocab: dict, medical_bag: dict, non_medical_bag: dict, medical_prob, non_medical_prob):\n",
    "    files = glob.glob(path)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for file in files:\n",
    "        likelihoods = [non_medical_prob, medical_prob]\n",
    "        with open(file, \"r\") as f: \n",
    "            data = f.read()\n",
    "             \n",
    "            file_BoW = normalize(data)\n",
    "            \n",
    "            # actual classification\n",
    "\n",
    "            for word in file_BoW:\n",
    "                if word in medical_bag:\n",
    "                    likelihoods[1] += np.log(medical_bag.get(word) / vocab.get(word))\n",
    "                if word in non_medical_bag:\n",
    "                    likelihoods[0] += np.log(non_medical_bag.get(word) / vocab.get(word))\n",
    "                \n",
    "            \n",
    "        labels.append(np.argmax(likelihoods))\n",
    "    \n",
    "    return labels       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:18.577356831Z",
     "start_time": "2023-11-16T19:38:18.512417223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def vocabulary(medical_BoW: dict, non_medical_BoW: dict):\n",
    "    vocab = {}\n",
    "    for word, count in medical_BoW.items():\n",
    "        vocab.update({word:count})\n",
    "    \n",
    "    for word, count in non_medical_BoW.items():\n",
    "        if word not in vocab.keys():\n",
    "            vocab.update({word:count})\n",
    "        else:\n",
    "            vocab.update({word:vocab.get(word) + count})\n",
    "        \n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:20.549622858Z",
     "start_time": "2023-11-16T19:38:20.537719094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Culture\n",
      "Scraping page: 19159508\n",
      "Scraping page: 24723521\n",
      "Scraping page: 72135653\n",
      "Scraping page: 74649989\n",
      "Scraping page: 18290472\n",
      "Scraping page: 29560452\n",
      "Scraping page: 53169305\n",
      "Scraping page: 505730\n",
      "Scraping page: 43569192\n",
      "Scraping page: 6258\n",
      "Scraping page: 590768\n",
      "Scraping page: 25147220\n",
      "Scraping page: 18964621\n",
      "Scraping page: 30963584\n",
      "Scraping page: 54004404\n",
      "Scraping page: 50693529\n",
      "Scraping page: 67233436\n",
      "Scraping page: 164660\n",
      "Scraping page: 69411572\n",
      "Scraping page: 4543340\n",
      "Scraping page: 66428540\n",
      "Scraping page: 2036118\n",
      "Scraping page: 57165694\n",
      "Scraping page: 9057549\n",
      "Scraping page: 5903\n",
      "Scraping page: 30487581\n",
      "Scraping page: 14690776\n",
      "Scraping page: 9020225\n",
      "Scraping page: 13144407\n",
      "Scraping page: 33301100\n",
      "Scraping page: 9216811\n",
      "Scraping page: 7745490\n",
      "Scraping page: 12593785\n",
      "Scraping page: 143364\n",
      "Scraping page: 1654632\n",
      "Scraping page: 42730418\n",
      "Scraping page: 32962014\n",
      "Scraping page: 60852572\n",
      "Scraping page: 12401182\n",
      "Scraping page: 62379378\n",
      "Scraping page: 323912\n",
      "Scraping page: 13775689\n",
      "Scraping page: 167703\n",
      "Scraping page: 25907070\n",
      "Scraping page: 8135793\n",
      "Scraping page: 36791036\n",
      "Scraping page: 1396834\n",
      "Scraping page: 3039067\n",
      "Scraping page: 1525258\n",
      "Scraping page: 69491097\n",
      "Scraping page: 73512780\n",
      "Scraping page: 677443\n",
      "Scraping page: 563299\n",
      "Scraping page: 50951733\n",
      "Scraping page: 13831\n",
      "Scraping page: 13598464\n",
      "Scraping page: 477975\n",
      "Scraping page: 6267\n",
      "Scraping page: 33596709\n",
      "Scraping page: 861492\n",
      "Scraping page: 782895\n",
      "Scraping page: 32017750\n",
      "Scraping page: 12870646\n",
      "Scraping page: 190041\n",
      "Scraping page: 1123773\n",
      "Scraping page: 2490371\n",
      "Scraping page: 8950930\n",
      "Scraping page: 1525262\n",
      "Scraping page: 18841893\n",
      "Scraping page: 22810417\n",
      "Scraping page: 70094520\n",
      "Scraping page: 63690634\n",
      "Scraping page: 34454406\n",
      "Scraping page: 70565448\n",
      "Scraping page: 1713306\n",
      "Scraping page: 25414\n",
      "Scraping page: 41748961\n",
      "Scraping page: 35291011\n",
      "Scraping page: 37886950\n",
      "Scraping page: 26642577\n",
      "Scraping page: 25778403\n",
      "Scraping page: 73999006\n",
      "Scraping page: 32839108\n",
      "Scraping page: 42534554\n",
      "Scraping page: 38566488\n",
      "Scraping page: 30049818\n",
      "Scraping page: 4016688\n",
      "Category:Literature\n",
      "Scraping page: 18963870\n",
      "Scraping page: 18345\n",
      "Scraping page: 75259373\n",
      "Scraping page: 1419427\n",
      "Scraping page: 297267\n",
      "Scraping page: 470031\n",
      "Scraping page: 13557443\n",
      "Scraping page: 33563052\n",
      "Scraping page: 649720\n",
      "Scraping page: 37389994\n",
      "Scraping page: 27691317\n",
      "Scraping page: 13607556\n",
      "Scraping page: 42431426\n",
      "Scraping page: 46382079\n",
      "Scraping page: 24145205\n",
      "Scraping page: 25590689\n",
      "Scraping page: 19284595\n",
      "Scraping page: 20395887\n",
      "Scraping page: 70575453\n",
      "Scraping page: 47243509\n",
      "Scraping page: 24616694\n",
      "Scraping page: 291229\n",
      "Scraping page: 2234574\n",
      "Scraping page: 58455529\n",
      "Scraping page: 181117\n",
      "Scraping page: 24587014\n",
      "Scraping page: 1001254\n",
      "Scraping page: 17046772\n",
      "Scraping page: 1851712\n",
      "Scraping page: 983787\n",
      "Scraping page: 15944015\n",
      "Scraping page: 455547\n",
      "Scraping page: 62712050\n",
      "Scraping page: 3837845\n",
      "Scraping page: 350381\n",
      "Scraping page: 722906\n",
      "Scraping page: 21424551\n",
      "Scraping page: 73826791\n",
      "Scraping page: 4137340\n",
      "Scraping page: 2119393\n",
      "Scraping page: 646598\n",
      "Scraping page: 8307635\n",
      "Scraping page: 30792823\n",
      "Scraping page: 46629670\n",
      "Scraping page: 62971006\n",
      "Scraping page: 1123773\n",
      "Scraping page: 26184869\n",
      "Scraping page: 54840463\n",
      "Scraping page: 39127991\n",
      "Scraping page: 70174\n",
      "Scraping page: 718763\n",
      "Scraping page: 75081155\n",
      "Scraping page: 2588267\n",
      "Scraping page: 46673516\n",
      "Scraping page: 2110568\n",
      "Scraping page: 97234\n",
      "Scraping page: 70991052\n",
      "Scraping page: 7410249\n",
      "Scraping page: 22926\n",
      "Scraping page: 20959122\n",
      "Scraping page: 1523896\n",
      "Scraping page: 23529\n",
      "Scraping page: 72772900\n",
      "Scraping page: 63224981\n",
      "Scraping page: 13666328\n",
      "Scraping page: 2694761\n",
      "Scraping page: 12749679\n",
      "Scraping page: 29784358\n",
      "Scraping page: 17569583\n",
      "Scraping page: 372984\n",
      "Scraping page: 302445\n",
      "Scraping page: 24991288\n",
      "Scraping page: 31896902\n",
      "Scraping page: 13878013\n",
      "Scraping page: 4444344\n",
      "Scraping page: 32923\n",
      "Scraping page: 19810565\n",
      "Scraping page: 4661812\n",
      "Scraping page: 1577499\n",
      "Category:Hunting\n",
      "Scraping page: 38791\n",
      "Scraping page: 3672921\n",
      "Scraping page: 6520040\n",
      "Scraping page: 36502503\n",
      "Scraping page: 9288150\n",
      "Scraping page: 571478\n",
      "Scraping page: 3698189\n",
      "Scraping page: 90016\n",
      "Scraping page: 6065207\n",
      "Scraping page: 2368441\n",
      "Scraping page: 6446\n",
      "Scraping page: 1602860\n",
      "Scraping page: 22382060\n",
      "Scraping page: 47373041\n",
      "Scraping page: 67753658\n",
      "Scraping page: 40774741\n",
      "Scraping page: 31778378\n",
      "Scraping page: 55918127\n",
      "Scraping page: 4205756\n",
      "Scraping page: 1179643\n",
      "Scraping page: 34194700\n",
      "Scraping page: 21810610\n",
      "Scraping page: 47721676\n",
      "Scraping page: 17069844\n",
      "Scraping page: 47981125\n",
      "Scraping page: 23580290\n",
      "Scraping page: 569398\n",
      "Scraping page: 8925435\n",
      "Scraping page: 2657094\n",
      "Scraping page: 43935749\n",
      "Scraping page: 5091961\n",
      "Scraping page: 53475752\n",
      "Scraping page: 771717\n",
      "Scraping page: 384000\n",
      "Scraping page: 52262886\n",
      "Scraping page: 4143670\n",
      "Scraping page: 46195176\n",
      "Scraping page: 23120242\n",
      "Scraping page: 12596750\n",
      "Scraping page: 50178403\n",
      "Scraping page: 46353964\n",
      "Scraping page: 28620682\n",
      "Scraping page: 74331623\n",
      "Scraping page: 10429922\n",
      "Scraping page: 27292060\n",
      "Scraping page: 16886629\n",
      "Scraping page: 3541975\n",
      "Scraping page: 72766586\n",
      "Scraping page: 6992131\n",
      "Scraping page: 21613021\n",
      "Scraping page: 46344833\n",
      "Scraping page: 25946967\n",
      "Scraping page: 2561683\n",
      "Scraping page: 57166820\n",
      "Scraping page: 29068932\n",
      "Scraping page: 65693666\n",
      "Scraping page: 301008\n",
      "Scraping page: 1973781\n",
      "Scraping page: 26947190\n",
      "Scraping page: 69821321\n",
      "Scraping page: 9898130\n",
      "Scraping page: 70166280\n",
      "Scraping page: 1481009\n",
      "Scraping page: 185166\n",
      "Scraping page: 1553293\n",
      "Scraping page: 47193639\n",
      "Scraping page: 176732\n",
      "Scraping page: 31074444\n",
      "Scraping page: 341486\n",
      "Scraping page: 67445428\n",
      "Scraping page: 72019739\n",
      "Scraping page: 1913249\n",
      "Scraping page: 1235701\n",
      "Scraping page: 26555908\n",
      "Scraping page: 33609941\n",
      "Scraping page: 75482\n",
      "Scraping page: 39046087\n",
      "Scraping page: 54608917\n",
      "Category:Politics\n",
      "Scraping page: 22986\n",
      "Scraping page: 38211935\n",
      "Scraping page: 1710895\n",
      "Scraping page: 651934\n",
      "Scraping page: 45670539\n",
      "Scraping page: 24466231\n",
      "Scraping page: 1414677\n",
      "Scraping page: 1997407\n",
      "Scraping page: 64102083\n",
      "Scraping page: 362102\n",
      "Scraping page: 59626904\n",
      "Scraping page: 53105040\n",
      "Scraping page: 331299\n",
      "Scraping page: 8970\n",
      "Scraping page: 31668876\n",
      "Scraping page: 9457\n",
      "Scraping page: 15039439\n",
      "Scraping page: 46536040\n",
      "Scraping page: 68554701\n",
      "Scraping page: 73666297\n",
      "Scraping page: 55762700\n",
      "Scraping page: 64666045\n",
      "Scraping page: 63109943\n",
      "Scraping page: 13831\n",
      "Scraping page: 295248\n",
      "Scraping page: 14507041\n",
      "Scraping page: 68169025\n",
      "Scraping page: 82533\n",
      "Scraping page: 51913\n",
      "Scraping page: 16102540\n",
      "Scraping page: 19012\n",
      "Scraping page: 1906130\n",
      "Scraping page: 42601798\n",
      "Scraping page: 67026535\n",
      "Scraping page: 8078743\n",
      "Scraping page: 52241092\n",
      "Scraping page: 3010656\n",
      "Scraping page: 65182265\n",
      "Scraping page: 25714536\n",
      "Scraping page: 713877\n",
      "Scraping page: 60912658\n",
      "Scraping page: 29976943\n",
      "Scraping page: 61937871\n",
      "Scraping page: 50543430\n",
      "Scraping page: 34245192\n",
      "Scraping page: 40362512\n",
      "Scraping page: 28072640\n",
      "Scraping page: 52102822\n",
      "Scraping page: 15525934\n",
      "Scraping page: 68171627\n",
      "Scraping page: 23040\n",
      "Scraping page: 584318\n",
      "Scraping page: 67484579\n",
      "Scraping page: 67827964\n",
      "Scraping page: 12957185\n",
      "Scraping page: 63541112\n",
      "Scraping page: 2745873\n",
      "Scraping page: 41550219\n",
      "Scraping page: 1306405\n",
      "Scraping page: 57093743\n",
      "Scraping page: 34354177\n",
      "Scraping page: 24493\n",
      "Scraping page: 340401\n",
      "Scraping page: 216170\n",
      "Scraping page: 25084\n",
      "Scraping page: 13843720\n",
      "Scraping page: 1113966\n",
      "Scraping page: 416518\n",
      "Scraping page: 51582\n",
      "Scraping page: 1212240\n",
      "Scraping page: 62473758\n",
      "Scraping page: 68172693\n",
      "Scraping page: 12833512\n",
      "Scraping page: 66252\n",
      "Scraping page: 804191\n",
      "Scraping page: 59939132\n",
      "Scraping page: 10826158\n",
      "Scraping page: 47139271\n",
      "Scraping page: 41815078\n",
      "Scraping page: 69865234\n",
      "Scraping page: 3512364\n",
      "Scraping page: 300545\n",
      "Scraping page: 69084631\n",
      "Scraping page: 41278664\n",
      "Scraping page: 65321385\n",
      "Category:FashionCategory:Architecture\n"
     ]
    }
   ],
   "source": [
    "retrieve_documents()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:14:06.303070166Z",
     "start_time": "2023-11-16T19:12:42.242103263Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "medical_bag_of_words = bag_of_words('Corpora/Medical/*.txt')\n",
    "non_medical_bag_of_words = bag_of_words('Corpora/NonMedical/*.txt')\n",
    "\n",
    "medical_prior = 584 / 997\n",
    "non_medical_prior = 413 / 997\n",
    "\n",
    "vocabulary = vocabulary(medical_bag_of_words, non_medical_bag_of_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:41.579963069Z",
     "start_time": "2023-11-16T19:38:30.000718713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of correct labels is: 191, which yields an accuracy of 0.9597989949748744\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = classify(path='Test/TestSet/*.txt', medical_bag=medical_bag_of_words, \n",
    "                            non_medical_bag=non_medical_bag_of_words, vocab=vocabulary,\n",
    "                            medical_prob=medical_prior, non_medical_prob=non_medical_prior)\n",
    "\n",
    "true_labels = []\n",
    "with open('Test/test_labels.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        true_labels.append(eval(line))\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] == true_labels[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"The total number of correct labels is: {correct}, which yields an accuracy of {correct/len(predicted_labels)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:38:59.304433805Z",
     "start_time": "2023-11-16T19:38:56.212794357Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# popoulates the test set subtrating the 20% of training set elements\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "counts = [0, 0]\n",
    "medical_documents = os.listdir('Corpora/Medical')\n",
    "non_medical_documents = os.listdir('Corpora/NonMedical')\n",
    "\n",
    "counts[0] = len(os.listdir('Corpora/NonMedical'))\n",
    "counts[1] = len(os.listdir('Corpora/Medical'))\n",
    "\n",
    "numer_of_documents = np.sum(counts)\n",
    "probabilities = [counts[0]/numer_of_documents, counts[1]/numer_of_documents]\n",
    "\n",
    "for _ in range(int(numer_of_documents*0.2)):\n",
    "    medical = np.random.choice([False, True], p=probabilities)\n",
    "    \n",
    "    document_index = None\n",
    "    \n",
    "    if medical:\n",
    "        document_index = np.random.randint(0, counts[1])\n",
    "    else:\n",
    "        document_index = np.random.randint(0, counts[0])\n",
    "        \n",
    "    if medical:\n",
    "        shutil.move(f\"Corpora/Medical/{medical_documents[document_index]}\", f\"Test/TestSet/{medical_documents[document_index]}\")\n",
    "        with open(\"Test/test_labels.txt\", \"a\") as f:\n",
    "            f.write(\"1\\n\")\n",
    "        del medical_documents[document_index]\n",
    "        counts[1] -= 1\n",
    "    else:\n",
    "        shutil.move(f\"Corpora/NonMedical/{non_medical_documents[document_index]}\", f\"Test/TestSet/{non_medical_documents[document_index]}\")\n",
    "        with open(\"Test/test_labels.txt\", \"a\") as f:\n",
    "            f.write(\"0\\n\")\n",
    "        del non_medical_documents[document_index]\n",
    "        counts[0] -= 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:36:12.111015187Z",
     "start_time": "2023-11-16T19:36:12.061321102Z"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
